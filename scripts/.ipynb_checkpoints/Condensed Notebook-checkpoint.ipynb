{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0d5897",
   "metadata": {},
   "source": [
    "In this notebook, I aim to roll through an analysis across a single patient which can easily be looped for multiple patients. To do so, we will use the functions that are written out more explicitly in the step-by-step notebooks. \n",
    "\n",
    "**This is the one you should copy and edit for your own actual analyses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa7b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38fbd72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, linregress\n",
    "import pandas as pd\n",
    "from mne.preprocessing.bads import _find_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7208da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LFPAnalysis import lfp_preprocess_utils, sync_utils, analysis_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818fc598",
   "metadata": {},
   "source": [
    "## Load, pre-process and re-reference the neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae5d1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/MS007_MemBandit.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 1867007  =      0.000 ...  1823.249 secs...\n",
      "Could not find a match for rhplt9.\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 6759 samples (6.601 sec)\n",
      "\n",
      "Writing /sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif\n",
      "Closing /sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif\n",
      "[done]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/lfp_preprocess_utils.py:556: RuntimeWarning: This filename (/sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  mne_data.save(f'{load_path}/photodiode.fif', picks='dc1', overwrite=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a match for rhplt9.\n",
      "sEEG channel type selected for re-referencing\n",
      "Creating RawArray with float64 data, n_channels=114, n_times=1867008\n",
      "    Range : 0 ... 1867007 =      0.000 ...  1823.249 secs\n",
      "Ready.\n",
      "Added the following bipolar channels:\n",
      "lacas1-lmolf1, lacas10-lacas9, lacas12-lacas9, lacas2-lmolf1, lacas3-lmolf1, lacas4-lacas8, lacas5-lacas8, lacas6-lacas8, lacas7-lacas8, laglt1-lhplt5, laglt10-laglt6, laglt2-lhplt5, laglt3-lhplt5, laglt7-lhplt6, laglt8-laglt6, laglt9-laglt6, laimm1-laglt5, laimm13-laimm12, laimm2-laimm6, laimm3-lmolf6, laimm4-laimm8, laimm5-laimm6, laimm7-laimm6, lcmfo1-lcmfo4, lcmfo12-lcmfo10, lcmfo13-lcmfo10, lcmfo2-lcmfo4, lcmfo3-lcmfo4, lcmfo7-lcmfo6, lcmfo8-lcmfo6, lhplt1-laglt5, lhplt10-lhplt8, lhplt2-laglt5, lhplt3-laglt4, lhplt4-lhplt6, lhplt9-lhplt8, lmcms1-lmcms5, lmcms2-lmcms5, lmcms3-lmcms5, lmcms4-lmcms5, lmcms9-lmcms8, lmolf2-lmolf6, lmolf3-lmolf6, lmolf4-laimm6, lmolf5-laimm6, lmolf8-laimm6, lmtpt1-lhplt5, lmtpt2-lhplt5, lmtpt3-lhplt5, lmtpt4-lhplt5, lmtpt5-lhplt7, lmtpt6-lhplt8, lmtpt7-lhplt8, lmtpt8-lhplt8, lpcip1-lpcip4, lpcip11-lpcip10, lpcip2-lpcip5, racas1-rmolf4, racas11-racas10, racas2-rmolf4, racas4-racas6, racas7-racas5, racas8-racas10, raglt1-raglt4, raglt2-raglt4, raglt3-raglt4, raglt6-raglt5, raglt7-raglt8, raglt9-rhplt8, raimm1-raglt5, raimm11-racas12, raimm12-racas12, raimm2-raimm5, raimm3-rmolf8, raimm4-rmolf8, raimm6-rmolf8, raimm7-racas9, raimm8-racas10, rcmfo1-rcmfo5, rcmfo10-rcmfo5, rcmfo11-rcmfo5, rcmfo12-raimm5, rcmfo13-raimm5, rcmfo2-rcmfo5, rcmfo3-rcmfo5, rcmfo4-rcmfo5, rcmfo7-rcmfo5, rcmfo8-rcmfo5, rcmfo9-rcmfo5, rhplt1-raglt4, rhplt2-rhplt4, rhplt3-rhplt4, rmcms2-rmcms7, rmcms3-rmcms7, rmcms4-rmcms1, rmcms5-rmcms8, rmcms6-rmcms7, rmcms9-rmcms7, rmolf1-racas3, rmolf2-racas3, rmolf5-rmolf8, rmolf6-rmolf3, rmolf7-rmolf3, rmolf9-rmolf3, rmtpt1-rmtpt3, rmtpt2-rmtpt3, rmtpt6-rmtpt4, rmtpt7-rmtpt4, rmtpt8-rmtpt4, rpcip1-rpcip5, rpcip11-rpcip8, rpcip2-rpcip5, rpcip7-rpcip6, rpcip9-rpcip8\n",
      "Overwriting existing file.\n",
      "Writing /sc/arion/work/qasims01/MemoryBanditData/EMU/Subjects/MS007/wm_ref_ieeg.fif\n",
      "Closing /sc/arion/work/qasims01/MemoryBanditData/EMU/Subjects/MS007/wm_ref_ieeg.fif\n",
      "[done]\n",
      "Opening raw data file /sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 1867007 =      0.000 ...  1823.249 secs\n",
      "Ready.\n",
      "Reading 0 ... 1867007  =      0.000 ...  1823.249 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100833/888861099.py:37: RuntimeWarning: This filename (/sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  photodiode_dict[subj_id].append(mne.io.read_raw_fif(f'{load_path}/photodiode.fif', preload=True))\n"
     ]
    }
   ],
   "source": [
    "base_dir = '/sc/arion' # this is the root directory for most un-archived data and results  \n",
    "subj_ids = ['MS007']\n",
    "elec_dict = {f'{x}': [] for x in subj_ids}\n",
    "mne_dict = {f'{x}': [] for x in subj_ids}\n",
    "photodiode_dict = {f'{x}': [] for x in subj_ids}\n",
    "for subj_id in subj_ids: \n",
    "    # Set paths\n",
    "    load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "    elec_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/anat/'\n",
    "    elec_files = glob(f'{elec_path}/*labels.csv')[0]\n",
    "    save_path = f'{base_dir}/work/qasims01/MemoryBanditData/EMU/Subjects/{subj_id}'\n",
    "    \n",
    "    # Load electrode data (should already be manually localized!)\n",
    "    elec_data = pd.read_csv(elec_files)\n",
    "\n",
    "    # Sometimes there's extra columns with no entries: \n",
    "    elec_data = elec_data[elec_data.columns.drop(list(elec_data.filter(regex='Unnamed')))]\n",
    "\n",
    "    # Load neural data\n",
    "    mne_data = lfp_preprocess_utils.make_mne(load_path=load_path, \n",
    "                                             save_path=save_path, \n",
    "                                             elec_data=elec_data, \n",
    "                                             format='edf')\n",
    "    \n",
    "    # Re-reference neural data\n",
    "    mne_data_reref = lfp_preprocess_utils.ref_mne(mne_data=mne_data, \n",
    "                                                  elec_data=elec_data, \n",
    "                                                  method='wm', \n",
    "                                                  site='MSSM')\n",
    "    \n",
    "    # Save this data so that you don't need this step again:\n",
    "    mne_data_reref.save(f'{save_path}/wm_ref_ieeg.fif', overwrite=True)\n",
    "\n",
    "    \n",
    "    # Append to list \n",
    "    mne_dict[subj_id].append(mne_data_reref)\n",
    "    \n",
    "    photodiode_dict[subj_id].append(mne.io.read_raw_fif(f'{load_path}/photodiode.fif', preload=True))\n",
    "    \n",
    "    elec_dict[subj_id].append(elec_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3821a0",
   "metadata": {},
   "source": [
    "## Extract behavioral information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd95dc5",
   "metadata": {},
   "source": [
    "Here, one should load in their own functions for behavioral stuff. I'll just write the functions relevant to me here for demonstration purposes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0599a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for image memorability ratings. \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "from scipy.stats import norm, zscore, linregress\n",
    "\n",
    "# Note: Much of the following is ported from: https://github.com/cvzoya/memorability-distinctiveness\n",
    "\n",
    "def dprime(pHit, pFA, PresentT, AbsentT, criteria=False):\n",
    "    \"\"\"\n",
    "    Note: from: http://nikos-konstantinou.blogspot.com/2010/02/dprime-function-in-matlab.html\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pHit : float\n",
    "        The proportion of \"Hits\": P(Yes|Signal)\n",
    "    pFA : float\n",
    "        The proportion of \"False Alarms\": P(Yes|Noise)\n",
    "    PresentT : int\n",
    "        The number of Signal Present Trials e.g. length(find(signal==1))\n",
    "    AbsentT : int\n",
    "        The number of Signal Absent Trials e.g. length(find(signal==0))\n",
    "\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dPrime: float\n",
    "        signal detection theory sensitivity measure \n",
    "    \n",
    "    beta: float\n",
    "        optional criterion value\n",
    "        \n",
    "    C: float\n",
    "        optional criterion value\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    if pHit == 1: \n",
    "        # if 100% Hits\n",
    "        pHit = 1 - (1/(2*PresentT))\n",
    "    \n",
    "    if pFA == 0: \n",
    "        # if 0% FA \n",
    "        pFA = 1/(2*AbsentT)\n",
    "        \n",
    "    # Convert to Z-scores\n",
    "    \n",
    "    zHit = norm.ppf(pHit) \n",
    "    zFA = norm.ppf(pFA) \n",
    "    \n",
    "    # calculate d-prime \n",
    "    \n",
    "    dPrime = zHit - zFA \n",
    "    \n",
    "    if criteria:\n",
    "        beta = np.exp((zFA**2 - zHit**2)/2)\n",
    "        C = -0.5 * (zHit + zFA)    \n",
    "        return dPrime, beta, C\n",
    "    else:\n",
    "        return dPrime\n",
    "\n",
    "def compute_memorability_scores(hits, false_alarms, misses, correct_rejections):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    hits : array-like\n",
    "        TODO\n",
    "    false_alarms : array-like\n",
    "        TODO\n",
    "    misses : array_like \n",
    "        TODO\n",
    "    correct_rejections : array_like \n",
    "        TODO\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    memory_ratings : pandas DataFrame \n",
    "        DataFrame with the following ratings added: HR (hit rate), FAR (false alarm rate), ACC (accuracy), DPRIME (d-prime), MI (mutual information)\n",
    "    \"\"\"\n",
    "\n",
    "    len_args = [len(hits), len(false_alarms), len(misses), len(correct_rejections)]\n",
    "    if not all(len_args[0] == _arg for _arg in len_args[1:]):\n",
    "            raise ValueError(\"All parameters must be the same length.\")\n",
    "    \n",
    "    memory_ratings = pd.DataFrame(columns = ['HR', 'FAR', 'ACC', 'DPRIME'])\n",
    "\n",
    "    nstimuli = len(hits) \n",
    "\n",
    "    hm = hits+misses\n",
    "    fc = false_alarms+correct_rejections\n",
    "\n",
    "    hrs = hits/hm\n",
    "    fars = false_alarms/fc\n",
    "    accs = (hits+correct_rejections)/(hm+fc)\n",
    "\n",
    "    dp = []\n",
    "    for i in range(nstimuli):\n",
    "        dp.append(dprime(hrs[i], fars[i], hm[i], fc[i]))\n",
    "\n",
    "    memory_ratings['HR'] = hrs\n",
    "    memory_ratings['FAR'] = fars\n",
    "    memory_ratings['ACC'] = accs\n",
    "    memory_ratings['DPRIME'] = dp\n",
    "    \n",
    "    return memory_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8f0e1",
   "metadata": {},
   "source": [
    "In order to analyze the neural data with respect to the behavioral data we need to be able to synchronize the two using the photodiode (or TTLs, eventually?) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4a6092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 485 neural syncs detected\n",
      "There are 486 behav syncs detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100833/1844076743.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mb_df.Gender[mb_df.Gender==0] = 2\n",
      "/tmp/ipykernel_100833/1844076743.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mb_df.Gender[mb_df.Gender==0] = 2\n",
      "/tmp/ipykernel_100833/1844076743.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mb_df['choice'] = mb_df.apply(lambda x: x['MB1_draw_key.keys'], axis=1)\n",
      "/tmp/ipykernel_100833/1844076743.py:163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mb_df['trials_dm'] = mb_df['trials_2.thisN'].shift(+1)\n",
      "/tmp/ipykernel_100833/1844076743.py:168: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mb_df.reward[mb_df.reward==100] = 0\n",
      "/tmp/ipykernel_100833/1844076743.py:201: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df['hit'] = 0\n",
      "/tmp/ipykernel_100833/1844076743.py:202: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df['miss'] = 0\n",
      "/tmp/ipykernel_100833/1844076743.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df['corr_reject'] = 0\n",
      "/tmp/ipykernel_100833/1844076743.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df['false_alarm'] = 0\n",
      "/tmp/ipykernel_100833/1844076743.py:207: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.rename(columns={'MEM2_recall_key.keys': 'response',\n",
      "/tmp/ipykernel_100833/1844076743.py:211: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df['trials_mem'] = rm_df['MEM2_trials.thisN'].shift(-1)\n",
      "/tmp/ipykernel_100833/1844076743.py:212: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.trials_mem.fillna(120, inplace=True)\n",
      "/tmp/ipykernel_100833/1844076743.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.Gender[rm_df.Gender==0] = 2\n",
      "/tmp/ipykernel_100833/1844076743.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.Gender[rm_df.Gender==0] = 2\n",
      "/tmp/ipykernel_100833/1844076743.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df = rm_df.merge(mb_df, on='img_path', how='left', indicator=True)\n",
      "/tmp/ipykernel_100833/1844076743.py:235: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.hit[hit_bool] = 1\n",
      "/tmp/ipykernel_100833/1844076743.py:236: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.miss[miss_bool] = 1\n",
      "/tmp/ipykernel_100833/1844076743.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.false_alarm[false_alarm_bool] = 1\n",
      "/tmp/ipykernel_100833/1844076743.py:238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.corr_reject[corr_reject_bool] = 1\n",
      "/tmp/ipykernel_100833/1844076743.py:260: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df['hit_rate'] = hrs\n",
      "/tmp/ipykernel_100833/1844076743.py:261: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df['false_alarm_rate'] = fars\n",
      "/tmp/ipykernel_100833/1844076743.py:262: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df['subj_dprime'] = np.nan\n",
      "/tmp/ipykernel_100833/1844076743.py:267: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df['DPRIME'] = rm_df.merge(all_mem_df, on='img_path', how='right')['DPRIME']\n",
      "/tmp/ipykernel_100833/1844076743.py:284: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df['phit'] = rm_df.response - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 blocks\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . found matches for 36 of 50 blocks\n",
      "sync succeeded\n"
     ]
    }
   ],
   "source": [
    "slopes = {f'{x}': [] for x in subj_ids}\n",
    "offsets = {f'{x}': [] for x in subj_ids}\n",
    "\n",
    "bandit_evs = {f'{x}': [] for x in subj_ids}\n",
    "memory_evs = {f'{x}': [] for x in subj_ids}\n",
    "\n",
    "for subj_id in subj_ids:\n",
    "    # Set paths\n",
    "    behav_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/behav/Day1'\n",
    "\n",
    "    # Find the timestamps of ONSET and OFFSET of all the sync signals in the photodiode \n",
    "    # moving average helps us detect the deflections \n",
    "    sig = np.squeeze(sync_utils.moving_average(photodiode_dict[subj_id][0]._data, n=11))\n",
    "    timestamp = np.squeeze(np.arange(len(sig))/mne_dict[subj_id][0].info['sfreq'])\n",
    "    # normalize\n",
    "    sig =  zscore(sig)\n",
    "    # look for z-scores above 1\n",
    "    trig_ix = np.where((sig[:-1]<=0)*(sig[1:]>0))[0] # rising edge of trigger\n",
    "    neural_ts = timestamp[trig_ix]\n",
    "    neural_ts = np.array(neural_ts)\n",
    "    print(f'There are {len(neural_ts)} neural syncs detected')\n",
    "    \n",
    "    # Get the .log file and/or .csv file, depending on how your task logs the behavioral data. Eventually this should be fairly standardized across tasks.\n",
    "\n",
    "    log_path = glob(f'{behav_path}/*.log')[0]\n",
    "    csv_path = glob(f'{behav_path}/*MB_MEM*.csv')[0]\n",
    "    \n",
    "    # Now get the relevant timestamps from behavioral logfiles. This will differ depending\n",
    "\n",
    "    MB1_ts = {'trial_start': [], \n",
    "    'deck_start': [], \n",
    "    'feedback_start': [],\n",
    "    'ITI_start': [],\n",
    "    'ITI_stop': []}\n",
    "\n",
    "    MEM2_ts = {'trial_start': [], \n",
    "    'face_start': [], \n",
    "    'slider_start': [],\n",
    "    'slider_stop': [],\n",
    "    'ITI_start': [],\n",
    "    'ITI_stop': []}\n",
    "\n",
    "    beh_ts = []\n",
    "\n",
    "    MB1_FLAG = True \n",
    "    MEM2_FLAG = False \n",
    "\n",
    "    with open(log_path, 'r') as fobj:\n",
    "        for ix, line in enumerate(fobj.readlines()):\n",
    "            line = line.replace('\\r', '')\n",
    "            tokens = line[:-1].split('\\t')\n",
    "\n",
    "            if tokens[1] == 'EXP ':\n",
    "                # Determine which task we are looking at \n",
    "                if tokens[2][0:3] == 'MB1':\n",
    "                    MB1_FLAG = True\n",
    "                    MEM2_FLAG = False \n",
    "                elif tokens[2][0:3] == 'MEM':\n",
    "                    MEM2_FLAG = True\n",
    "                    MB1_FLAG = False\n",
    "\n",
    "                # Grab photodiode timestamp\n",
    "                if tokens[2][0:4] =='sync':\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        beh_ts.append(float(tokens[0]))\n",
    "\n",
    "                # Get MB1 deck \n",
    "                if 'MB1_left_draw' in tokens[2]:\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MB1_ts['deck_start'].append(float(tokens[0]))\n",
    "\n",
    "                # Get MB1 feedback\n",
    "                if 'MB1_face' in tokens[2]:\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MB1_ts['feedback_start'].append(float(tokens[0]))\n",
    "\n",
    "                # Get MB1 ITI cross \n",
    "                if 'MB1_ITI_cross' in tokens[2]:\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MB1_ts['ITI_start'].append(float(tokens[0]))\n",
    "                    elif 'autoDraw = False' in tokens[2]:\n",
    "                        MB1_ts['ITI_stop'].append(float(tokens[0]))\n",
    "\n",
    "                if 'New trial (rep=0' in tokens[2]:\n",
    "                    if MB1_FLAG: \n",
    "                        # remember to discard the first one later - it's pre-session \n",
    "                        MB1_ts['trial_start'].append(float(tokens[0]))\n",
    "                    elif MEM2_FLAG:\n",
    "                        MEM2_ts['trial_start'].append(float(tokens[0]))\n",
    "\n",
    "                # Get MEM2 ITI\n",
    "                if 'MEM2_jitter' in tokens[2]:\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MEM2_ts['ITI_start'].append(float(tokens[0]))          \n",
    "                    elif 'autoDraw = False' in tokens[2]:\n",
    "                        MEM2_ts['ITI_stop'].append(float(tokens[0]))  \n",
    "\n",
    "                # Get MEM2 Face\n",
    "                if 'MEM2_images' in tokens[2]:\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MEM2_ts['face_start'].append(float(tokens[0]))     \n",
    "\n",
    "                # Get MEM2 slider start\n",
    "                if tokens[2][:16] == 'MEM2_conf_slider':\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MEM2_ts['slider_start'].append(float(tokens[0]))    \n",
    "\n",
    "                 # Get MEM2 slider stop\n",
    "                if tokens[2][:16] == 'MEM2_conf_slider':\n",
    "                    if 'autoDraw = False' in tokens[2]:\n",
    "                        MEM2_ts['slider_stop'].append(float(tokens[0]))                                               \n",
    "\n",
    "    beh_ts = np.array(beh_ts)\n",
    "    print(f'There are {len(beh_ts)} behav syncs detected')\n",
    "\n",
    "    # Note: fixation crosses need fixing on stop time duplicates\n",
    "    MB1_ts['ITI_stop'] = np.unique(MB1_ts['ITI_stop']).tolist()\n",
    "    MEM2_ts['ITI_stop'] = np.unique(MEM2_ts['ITI_stop']).tolist()\n",
    "\n",
    "    # Get the choice times: \n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "    MB1_ts['choice'] = (csv_data['MB1_draw_key.started'].dropna() + csv_data['MB1_draw_key.rt'].dropna()).tolist()\n",
    "    MEM2_ts['choice'] = (csv_data['MEM2_recall_key.started'].dropna() + csv_data['MEM2_recall_key.rt'].dropna()).tolist()\n",
    "\n",
    "    # Do some corrections: \n",
    "    # Get rid of first trial start (pre-session)\n",
    "    MB1_ts['trial_start'].pop(0) \n",
    "    \n",
    "    subj_count = 0\n",
    "\n",
    "    # Load the database with image DPRIME information\n",
    "    database_file = f'{behav_path}/all_mem_data.xlsx' \n",
    "    all_mem_df = pd.read_excel(database_file, engine='openpyxl')\n",
    "    all_mem_df = all_mem_df[['img_path', 'DPRIME']]\n",
    "\n",
    "    # Turn into right format for modeling: \n",
    "    MB1_n = 60\n",
    "    MEM2_n = 120\n",
    "    li_mb1 = []\n",
    "    li_mem2 = [] \n",
    "\n",
    "    act_rew_rate = {}\n",
    "    act_rew_rate['pids'] = []\n",
    "\n",
    "    r1_chance=30\n",
    "\n",
    "    for elem in ['actions', 'rewards']:\n",
    "        act_rew_rate[elem] = np.zeros(MB1_n).astype(int) # len(task_files), \n",
    "\n",
    "    # Load the merged task data \n",
    "    csv_data['trials_2.thisN'] = csv_data['trials_2.thisRepN'].shift(-1)\n",
    "\n",
    "    ##### First, process the Bandit task: \n",
    "    mb_df = csv_data.dropna(subset=['trials_2.thisN'])\n",
    "    act_rew_rate['pids'].append(mb_df.participant.iloc[0])\n",
    "\n",
    "    # Change Gender so that female = 2\n",
    "    mb_df.Gender[mb_df.Gender==0] = 2\n",
    "\n",
    "    # add score, reward probability and expected value \n",
    "    mb_df['choice'] = mb_df.apply(lambda x: x['MB1_draw_key.keys'], axis=1)\n",
    "    # Make the trials 1-60 \n",
    "    mb_df['trials_dm'] = mb_df['trials_2.thisN'].shift(+1)\n",
    "    # mb_df.trials_dm.fillna(60, inplace=True)\n",
    "    # # get rid of the extra rows in the .csv that populate between trials \n",
    "    mb_df = mb_df.drop_duplicates(subset='trials_dm', keep='first')\n",
    "    mb_df['reward'] = mb_df.apply(lambda x: x['reward']/100, axis=1)\n",
    "    mb_df.reward[mb_df.reward==100] = 0\n",
    "    # 0 is male, 1 is female \n",
    "    mb_df['choice'] = mb_df['choice']-1\n",
    "    mb_df.rename(columns={'MB1_draw_key.rt':'draw_rt'}, inplace=True)\n",
    "    mb_df.dropna(subset=['img_path'], inplace=True)\n",
    "\n",
    "    ##### Fit RW model to DM data\n",
    "    mb_df['bic'] = np.nan\n",
    "    mb_df['alpha']  = np.nan\n",
    "    mb_df['beta']  = np.nan\n",
    "    mb_df['RPE']  = np.nan\n",
    "\n",
    "    # RW_model = RW() \n",
    "    sub_act_rew_rate = {}\n",
    "    sub_act_rew_rate['pids'] = np.array([subj_count])  \n",
    "\n",
    "    for elem in ['actions', 'rewards']:\n",
    "        sub_act_rew_rate[elem] = np.zeros([1, MB1_n]).astype(int)\n",
    "    c = mb_df.choice.dropna().values.astype(int)\n",
    "    r = mb_df.reward.dropna().values\n",
    "    sub_act_rew_rate['actions'][0, :] = c\n",
    "    sub_act_rew_rate['rewards'][0, :] = r\n",
    "\n",
    "    # Save dict for modeling decision-making performance: \n",
    "    c = mb_df.choice.dropna().values.astype(int)\n",
    "    r = mb_df.reward.dropna().values\n",
    "    act_rew_rate['actions'] = c # [subj_count, :]\n",
    "    act_rew_rate['rewards']= r # [subj_count, :]\n",
    "\n",
    "    ##### Second, process the MEM2 data: \n",
    "    rm_df = csv_data.dropna(subset=['MEM2_trials.thisN'])\n",
    "\n",
    "    # add coding of memory choice: \n",
    "    rm_df['hit'] = 0\n",
    "    rm_df['miss'] = 0\n",
    "    rm_df['corr_reject'] = 0\n",
    "    rm_df['false_alarm'] = 0\n",
    "\n",
    "    # add score, reward probability and expected value \n",
    "    rm_df.rename(columns={'MEM2_recall_key.keys': 'response',\n",
    "    'MEM2_conf_slider.response': 'confidence'\n",
    "     }, inplace=True) \n",
    "\n",
    "    rm_df['trials_mem'] = rm_df['MEM2_trials.thisN'].shift(-1)\n",
    "    rm_df.trials_mem.fillna(120, inplace=True)\n",
    "\n",
    "    # Change Gender so that female = 2\n",
    "    rm_df.Gender[rm_df.Gender==0] = 2\n",
    "\n",
    "    rm_df = rm_df.merge(mb_df, on='img_path', how='left', indicator=True)\n",
    "    # Clean up the merge\n",
    "    rm_df.drop(columns=['participant_y'], inplace=True)\n",
    "    rm_df.rename(columns={'participant_x': 'participant'}, inplace=True)\n",
    "\n",
    "    hit_bool = (rm_df._merge=='both') & (rm_df.response==2)\n",
    "    hits = hit_bool.sum()\n",
    "    # NEW = 1, which is false in this case \n",
    "    miss_bool = (rm_df._merge=='both') & (rm_df.response==1)\n",
    "    misses = miss_bool.sum()\n",
    "\n",
    "    # or just the \"left\" df ('new')\n",
    "    false_alarm_bool = (rm_df._merge=='left_only') & (rm_df.response==2)\n",
    "    false_alarms = false_alarm_bool.sum()\n",
    "    corr_reject_bool = (rm_df._merge=='left_only') & (rm_df.response==1)\n",
    "    correct_rejections = corr_reject_bool.sum()\n",
    "\n",
    "    # categorize image by memory choice\n",
    "    rm_df.hit[hit_bool] = 1\n",
    "    rm_df.miss[miss_bool] = 1\n",
    "    rm_df.false_alarm[false_alarm_bool] = 1\n",
    "    rm_df.corr_reject[corr_reject_bool] = 1\n",
    "\n",
    "    # compute dprime for the subject\n",
    "    hm = hits+misses\n",
    "    fc = false_alarms+correct_rejections\n",
    "\n",
    "    hrs = hits/hm\n",
    "    fars = false_alarms/fc\n",
    "\n",
    "    # Adjust extreme hit-rates or false-alarms\n",
    "    if hrs == 0: \n",
    "        hrs = 0.5/hm\n",
    "    elif hrs ==1: \n",
    "        hrs = (hm-0.5)/hm\n",
    "    if fars == 0: \n",
    "        fars = 0.5/fc\n",
    "    elif fars ==1: \n",
    "        fars = (fc-0.5)/fc\n",
    "\n",
    "    dp = dprime(hrs, fars, hm, fc)\n",
    "\n",
    "    # Add in subject-level memory characteristics (\"rates\")\n",
    "    rm_df['hit_rate'] = hrs\n",
    "    rm_df['false_alarm_rate'] = fars\n",
    "    rm_df['subj_dprime'] = np.nan\n",
    "    if dp != float(\"-inf\"):\n",
    "        rm_df['subj_dprime'] = dp    \n",
    "\n",
    "    # Merge in the image DPRIME \n",
    "    rm_df['DPRIME'] = rm_df.merge(all_mem_df, on='img_path', how='right')['DPRIME']\n",
    "    mb_df['DPRIME'] = mb_df.merge(all_mem_df, on='img_path', how='right')['DPRIME']\n",
    "    mb_df.rename(columns={'DPRIME': 'image_dprime'}, inplace=True) \n",
    "\n",
    "    rm_df.rename(columns={'DPRIME': 'image_dprime',\n",
    "    'Gender_x': 'image_gender',\n",
    "    'MEM2_recall_key.rt_x': 'recall_rt',\n",
    "    'MEM2_conf_slider.rt_x': 'slider_rt'\n",
    "    }, inplace=True) \n",
    "\n",
    "\n",
    "    # dm_df = pd.concat(li_mb1, axis=0, ignore_index=True)\n",
    "    mb_df['male'] = 0\n",
    "    mb_df['female'] = 0\n",
    "    mb_df.male = mb_df.apply(lambda x: 1 if x.choice==0 else 0, axis=1)\n",
    "    mb_df.female = mb_df.apply(lambda x: 1 if x.choice==1 else 0, axis=1)\n",
    "\n",
    "    rm_df['phit'] = rm_df.response - 1\n",
    "\n",
    "    col_mask = ((rm_df.columns.str.startswith('MEM')) | (rm_df.columns.str.startswith('MB')) | (rm_df.columns.str.startswith('trials.')) | (rm_df.columns.str.startswith('trials_2')) | (rm_df.columns.str.endswith('_y')))\n",
    "    rm_df = rm_df.loc[:,~col_mask]\n",
    "\n",
    "    # Do regression to find neural timestamps for each event type\n",
    "    if len(beh_ts)!=len(neural_ts):\n",
    "        good_beh_ms, neural_offset = sync_utils.pulsealign(beh_ts, neural_ts, window=50, thresh=0.95)\n",
    "        slope, offset, rval = sync_utils.sync_matched_pulses(good_beh_ms, neural_offset)\n",
    "    else:\n",
    "        slope, offset, rval = sync_utils.sync_matched_pulses(beh_ts, neural_ts)\n",
    "\n",
    "    if rval < 0.99:\n",
    "        print('sync failed')\n",
    "    else: \n",
    "        print('sync succeeded')\n",
    "        \n",
    "    slopes[subj_id].append(slope)\n",
    "    offsets[subj_id].append(offset)\n",
    "    bandit_evs[subj_id].append(MB1_ts)\n",
    "    memory_evs[subj_id].append(MEM2_ts)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc40cd",
   "metadata": {},
   "source": [
    "## Make epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c88e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some windows of interest \n",
    "\n",
    "buf = 1.0 # this is the buffer before and after that we use to limit edge effects for TFRs\n",
    "\n",
    "feedback_pre = 1.0 # this is the time before the feedback appears \n",
    "feedback_post = 1.5 # this is the time the feedback is present \n",
    "\n",
    "IED_args = {'peak_thresh':4,\n",
    "           'closeness_thresh':0.25, \n",
    "           'width_thresh':0.2}\n",
    "\n",
    "# add behavioral times of interest \n",
    "for subj_id in subj_ids:\n",
    "    # Set paths\n",
    "    load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "    save_path = f'{base_dir}/work/qasims01/MemoryBanditData/EMU/Subjects/{subj_id}'\n",
    "\n",
    "    epochs = lfp_preprocess_utils.make_epochs(load_path=load_path, save_path=save_path, elec_data=elec_dict[subj_id][0], \n",
    "                                              slope=slopes[subj_id][0], offset=offsets[subj_id][0], \n",
    "                                              behav_times=bandit_evs[subj_id][0]['feedback_start'], \n",
    "                                              baseline_times=None, baseline_dur=None, fixed_baseline=[-1.0, 0],\n",
    "                                              buf_s=buf, pre_s=-feedback_pre, post_s=feedback_post, downsamp_factor=2, IED_args=IED_args)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd5685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0ec41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp_analysis",
   "language": "python",
   "name": "lfpanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
