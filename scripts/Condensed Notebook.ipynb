{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I aim to roll through an analysis across a single patient which can easily be looped for multiple patients. To do so, we will use the functions that are written out more explicitly in the step-by-step notebooks. \n",
    "\n",
    "**This is the one you should copy and edit for your own actual analyses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, linregress\n",
    "import pandas as pd\n",
    "from mne.preprocessing.bads import _find_outliers\n",
    "from fooof import FOOOFGroup\n",
    "from fooof.bands import Bands\n",
    "import os \n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you have installed this package in editable form on Minerva, you must append the local path! This is because Minerva requires that you point your package installs away from the local directory for space reasons, but editable packages have to be installed locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/hpc/users/qasims01/resources/LFPAnalysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LFPAnalysis import lfp_preprocess_utils, sync_utils, analysis_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, pre-process and re-reference the neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/sc/arion' # this is the root directory for most un-archived data and results  \n",
    "subj_ids = ['MS007']\n",
    "elec_dict = {f'{x}': [] for x in subj_ids}\n",
    "mne_dict = {f'{x}': [] for x in subj_ids}\n",
    "photodiode_dict = {f'{x}': [] for x in subj_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/MS007_MemBandit.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 1867007  =      0.000 ...  1823.249 secs...\n",
      "Could not find a match for rhplt9.\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 6759 samples (6.601 sec)\n",
      "\n",
      "Writing /sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif\n",
      "Closing /sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif\n",
      "[done]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/lfp_preprocess_utils.py:556: RuntimeWarning: This filename (/sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  mne_data.save(f'{load_path}/photodiode.fif', picks='dc1', overwrite=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a match for rhplt9.\n",
      "sEEG channel type selected for re-referencing\n",
      "Creating RawArray with float64 data, n_channels=114, n_times=1867008\n",
      "    Range : 0 ... 1867007 =      0.000 ...  1823.249 secs\n",
      "Ready.\n",
      "Added the following bipolar channels:\n",
      "lacas1-lmolf1, lacas10-lacas9, lacas12-lacas9, lacas2-lmolf1, lacas3-lmolf1, lacas4-lacas8, lacas5-lacas8, lacas6-lacas8, lacas7-lacas8, laglt1-lhplt5, laglt10-laglt6, laglt2-lhplt5, laglt3-lhplt5, laglt7-lhplt6, laglt8-laglt6, laglt9-laglt6, laimm1-laglt5, laimm13-laimm12, laimm2-laimm6, laimm3-lmolf6, laimm4-laimm8, laimm5-laimm6, laimm7-laimm6, lcmfo1-lcmfo4, lcmfo12-lcmfo10, lcmfo13-lcmfo10, lcmfo2-lcmfo4, lcmfo3-lcmfo4, lcmfo7-lcmfo6, lcmfo8-lcmfo6, lhplt1-laglt5, lhplt10-lhplt8, lhplt2-laglt5, lhplt3-laglt4, lhplt4-lhplt6, lhplt9-lhplt8, lmcms1-lmcms5, lmcms2-lmcms5, lmcms3-lmcms5, lmcms4-lmcms5, lmcms9-lmcms8, lmolf2-lmolf6, lmolf3-lmolf6, lmolf4-laimm6, lmolf5-laimm6, lmolf8-laimm6, lmtpt1-lhplt5, lmtpt2-lhplt5, lmtpt3-lhplt5, lmtpt4-lhplt5, lmtpt5-lhplt7, lmtpt6-lhplt8, lmtpt7-lhplt8, lmtpt8-lhplt8, lpcip1-lpcip4, lpcip11-lpcip10, lpcip2-lpcip5, racas1-rmolf4, racas11-racas10, racas2-rmolf4, racas4-racas6, racas7-racas5, racas8-racas10, raglt1-raglt4, raglt2-raglt4, raglt3-raglt4, raglt6-raglt5, raglt7-raglt8, raglt9-rhplt8, raimm1-raglt5, raimm11-racas12, raimm12-racas12, raimm2-raimm5, raimm3-rmolf8, raimm4-rmolf8, raimm6-rmolf8, raimm7-racas9, raimm8-racas10, rcmfo1-rcmfo5, rcmfo10-rcmfo5, rcmfo11-rcmfo5, rcmfo12-raimm5, rcmfo13-raimm5, rcmfo2-rcmfo5, rcmfo3-rcmfo5, rcmfo4-rcmfo5, rcmfo7-rcmfo5, rcmfo8-rcmfo5, rcmfo9-rcmfo5, rhplt1-raglt4, rhplt2-rhplt4, rhplt3-rhplt4, rmcms2-rmcms7, rmcms3-rmcms7, rmcms4-rmcms1, rmcms5-rmcms8, rmcms6-rmcms7, rmcms9-rmcms7, rmolf1-racas3, rmolf2-racas3, rmolf5-rmolf8, rmolf6-rmolf3, rmolf7-rmolf3, rmolf9-rmolf3, rmtpt1-rmtpt3, rmtpt2-rmtpt3, rmtpt6-rmtpt4, rmtpt7-rmtpt4, rmtpt8-rmtpt4, rpcip1-rpcip5, rpcip11-rpcip8, rpcip2-rpcip5, rpcip7-rpcip6, rpcip9-rpcip8\n",
      "Overwriting existing file.\n",
      "Writing /sc/arion/work/qasims01/MemoryBanditData/EMU/Subjects/MS007/wm_ref_ieeg.fif\n",
      "Closing /sc/arion/work/qasims01/MemoryBanditData/EMU/Subjects/MS007/wm_ref_ieeg.fif\n",
      "[done]\n",
      "Opening raw data file /sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 1867007 =      0.000 ...  1823.249 secs\n",
      "Ready.\n",
      "Reading 0 ... 1867007  =      0.000 ...  1823.249 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100833/888861099.py:37: RuntimeWarning: This filename (/sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  photodiode_dict[subj_id].append(mne.io.read_raw_fif(f'{load_path}/photodiode.fif', preload=True))\n"
     ]
    }
   ],
   "source": [
    "for subj_id in subj_ids: \n",
    "    # Set paths\n",
    "    load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "    elec_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/anat/'\n",
    "    elec_files = glob(f'{elec_path}/*labels.csv')[0]\n",
    "    save_path = f'{base_dir}/work/qasims01/MemoryBanditData/EMU/Subjects/{subj_id}'\n",
    "    \n",
    "    # Load electrode data (should already be manually localized!)\n",
    "    elec_data = pd.read_csv(elec_files)\n",
    "\n",
    "    # Sometimes there's extra columns with no entries: \n",
    "    elec_data = elec_data[elec_data.columns.drop(list(elec_data.filter(regex='Unnamed')))]\n",
    "\n",
    "    # Load neural data\n",
    "    mne_data = lfp_preprocess_utils.make_mne(load_path=load_path, \n",
    "                                             elec_data=elec_data, \n",
    "                                             format='edf')\n",
    "    \n",
    "    # Re-reference neural data\n",
    "    mne_data_reref = lfp_preprocess_utils.ref_mne(mne_data=mne_data, \n",
    "                                                  elec_data=elec_data, \n",
    "                                                  method='wm', \n",
    "                                                  site='MSSM')\n",
    "    \n",
    "    # Save this data so that you don't need this step again:\n",
    "    mne_data_reref.save(f'{save_path}/wm_ref_ieeg.fif', overwrite=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /sc/arion/work/qasims01/MemoryBanditData/EMU/Subjects/MS007/wm_ref_ieeg.fif...\n",
      "    Range : 0 ... 1867007 =      0.000 ...  1823.249 secs\n",
      "Ready.\n",
      "Reading 0 ... 1867007  =      0.000 ...  1823.249 secs...\n",
      "Opening raw data file /sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 1867007 =      0.000 ...  1823.249 secs\n",
      "Ready.\n",
      "Reading 0 ... 1867007  =      0.000 ...  1823.249 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123865/3753240891.py:17: RuntimeWarning: This filename (/sc/arion/projects/guLab/Salman/EMU/MS007/neural/Day1/photodiode.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  photodiode_data = mne.io.read_raw_fif(f'{load_path}/photodiode.fif', preload=True)\n"
     ]
    }
   ],
   "source": [
    "# If you have ran the preprocessing above, load the data instead: \n",
    "for subj_id in subj_ids: \n",
    "    # Set paths\n",
    "    load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "    elec_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/anat/'\n",
    "    elec_files = glob(f'{elec_path}/*labels.csv')[0]\n",
    "    save_path = f'{base_dir}/work/qasims01/MemoryBanditData/EMU/Subjects/{subj_id}'\n",
    "    \n",
    "    # Load electrode data (should already be manually localized!)\n",
    "    elec_data = pd.read_csv(elec_files)\n",
    "\n",
    "    # Sometimes there's extra columns with no entries: \n",
    "    elec_data = elec_data[elec_data.columns.drop(list(elec_data.filter(regex='Unnamed')))]\n",
    "\n",
    "    mne_data_reref = mne.io.read_raw_fif(f'{save_path}/wm_ref_ieeg.fif', preload=True)\n",
    "\n",
    "    photodiode_data = mne.io.read_raw_fif(f'{load_path}/photodiode.fif', preload=True)\n",
    "    \n",
    "    # Append to list \n",
    "    mne_dict[subj_id].append(mne_data_reref)\n",
    "    \n",
    "    photodiode_dict[subj_id].append(photodiode_data)\n",
    "    \n",
    "    elec_dict[subj_id].append(elec_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract behavioral information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, one should load in their own functions for behavioral stuff. I'll just write the functions relevant to me here for demonstration purposes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for image memorability ratings. \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "from scipy.stats import norm, zscore, linregress\n",
    "\n",
    "# Note: Much of the following is ported from: https://github.com/cvzoya/memorability-distinctiveness\n",
    "\n",
    "def dprime(pHit, pFA, PresentT, AbsentT, criteria=False):\n",
    "    \"\"\"\n",
    "    Note: from: http://nikos-konstantinou.blogspot.com/2010/02/dprime-function-in-matlab.html\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pHit : float\n",
    "        The proportion of \"Hits\": P(Yes|Signal)\n",
    "    pFA : float\n",
    "        The proportion of \"False Alarms\": P(Yes|Noise)\n",
    "    PresentT : int\n",
    "        The number of Signal Present Trials e.g. length(find(signal==1))\n",
    "    AbsentT : int\n",
    "        The number of Signal Absent Trials e.g. length(find(signal==0))\n",
    "\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dPrime: float\n",
    "        signal detection theory sensitivity measure \n",
    "    \n",
    "    beta: float\n",
    "        optional criterion value\n",
    "        \n",
    "    C: float\n",
    "        optional criterion value\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    if pHit == 1: \n",
    "        # if 100% Hits\n",
    "        pHit = 1 - (1/(2*PresentT))\n",
    "    \n",
    "    if pFA == 0: \n",
    "        # if 0% FA \n",
    "        pFA = 1/(2*AbsentT)\n",
    "        \n",
    "    # Convert to Z-scores\n",
    "    \n",
    "    zHit = norm.ppf(pHit) \n",
    "    zFA = norm.ppf(pFA) \n",
    "    \n",
    "    # calculate d-prime \n",
    "    \n",
    "    dPrime = zHit - zFA \n",
    "    \n",
    "    if criteria:\n",
    "        beta = np.exp((zFA**2 - zHit**2)/2)\n",
    "        C = -0.5 * (zHit + zFA)    \n",
    "        return dPrime, beta, C\n",
    "    else:\n",
    "        return dPrime\n",
    "\n",
    "def compute_memorability_scores(hits, false_alarms, misses, correct_rejections):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    hits : array-like\n",
    "        TODO\n",
    "    false_alarms : array-like\n",
    "        TODO\n",
    "    misses : array_like \n",
    "        TODO\n",
    "    correct_rejections : array_like \n",
    "        TODO\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    memory_ratings : pandas DataFrame \n",
    "        DataFrame with the following ratings added: HR (hit rate), FAR (false alarm rate), ACC (accuracy), DPRIME (d-prime), MI (mutual information)\n",
    "    \"\"\"\n",
    "\n",
    "    len_args = [len(hits), len(false_alarms), len(misses), len(correct_rejections)]\n",
    "    if not all(len_args[0] == _arg for _arg in len_args[1:]):\n",
    "            raise ValueError(\"All parameters must be the same length.\")\n",
    "    \n",
    "    memory_ratings = pd.DataFrame(columns = ['HR', 'FAR', 'ACC', 'DPRIME'])\n",
    "\n",
    "    nstimuli = len(hits) \n",
    "\n",
    "    hm = hits+misses\n",
    "    fc = false_alarms+correct_rejections\n",
    "\n",
    "    hrs = hits/hm\n",
    "    fars = false_alarms/fc\n",
    "    accs = (hits+correct_rejections)/(hm+fc)\n",
    "\n",
    "    dp = []\n",
    "    for i in range(nstimuli):\n",
    "        dp.append(dprime(hrs[i], fars[i], hm[i], fc[i]))\n",
    "\n",
    "    memory_ratings['HR'] = hrs\n",
    "    memory_ratings['FAR'] = fars\n",
    "    memory_ratings['ACC'] = accs\n",
    "    memory_ratings['DPRIME'] = dp\n",
    "    \n",
    "    return memory_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyze the neural data with respect to the behavioral data we need to be able to synchronize the two using the photodiode (or TTLs, eventually?) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 485 neural syncs detected\n",
      "There are 486 behav syncs detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123865/1844076743.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mb_df.Gender[mb_df.Gender==0] = 2\n",
      "/tmp/ipykernel_123865/1844076743.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mb_df.Gender[mb_df.Gender==0] = 2\n",
      "/tmp/ipykernel_123865/1844076743.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mb_df['choice'] = mb_df.apply(lambda x: x['MB1_draw_key.keys'], axis=1)\n",
      "/tmp/ipykernel_123865/1844076743.py:163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mb_df['trials_dm'] = mb_df['trials_2.thisN'].shift(+1)\n",
      "/tmp/ipykernel_123865/1844076743.py:168: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mb_df.reward[mb_df.reward==100] = 0\n",
      "/tmp/ipykernel_123865/1844076743.py:201: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df['hit'] = 0\n",
      "/tmp/ipykernel_123865/1844076743.py:202: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df['miss'] = 0\n",
      "/tmp/ipykernel_123865/1844076743.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df['corr_reject'] = 0\n",
      "/tmp/ipykernel_123865/1844076743.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df['false_alarm'] = 0\n",
      "/tmp/ipykernel_123865/1844076743.py:207: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.rename(columns={'MEM2_recall_key.keys': 'response',\n",
      "/tmp/ipykernel_123865/1844076743.py:211: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df['trials_mem'] = rm_df['MEM2_trials.thisN'].shift(-1)\n",
      "/tmp/ipykernel_123865/1844076743.py:212: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.trials_mem.fillna(120, inplace=True)\n",
      "/tmp/ipykernel_123865/1844076743.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.Gender[rm_df.Gender==0] = 2\n",
      "/tmp/ipykernel_123865/1844076743.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.Gender[rm_df.Gender==0] = 2\n",
      "/tmp/ipykernel_123865/1844076743.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df = rm_df.merge(mb_df, on='img_path', how='left', indicator=True)\n",
      "/tmp/ipykernel_123865/1844076743.py:235: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.hit[hit_bool] = 1\n",
      "/tmp/ipykernel_123865/1844076743.py:236: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.miss[miss_bool] = 1\n",
      "/tmp/ipykernel_123865/1844076743.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.false_alarm[false_alarm_bool] = 1\n",
      "/tmp/ipykernel_123865/1844076743.py:238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rm_df.corr_reject[corr_reject_bool] = 1\n",
      "/tmp/ipykernel_123865/1844076743.py:260: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df['hit_rate'] = hrs\n",
      "/tmp/ipykernel_123865/1844076743.py:261: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df['false_alarm_rate'] = fars\n",
      "/tmp/ipykernel_123865/1844076743.py:262: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df['subj_dprime'] = np.nan\n",
      "/tmp/ipykernel_123865/1844076743.py:267: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df['DPRIME'] = rm_df.merge(all_mem_df, on='img_path', how='right')['DPRIME']\n",
      "/tmp/ipykernel_123865/1844076743.py:284: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rm_df['phit'] = rm_df.response - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 blocks\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . found matches for 36 of 50 blocks\n",
      "sync succeeded\n"
     ]
    }
   ],
   "source": [
    "slopes = {f'{x}': [] for x in subj_ids}\n",
    "offsets = {f'{x}': [] for x in subj_ids}\n",
    "\n",
    "bandit_evs = {f'{x}': [] for x in subj_ids}\n",
    "memory_evs = {f'{x}': [] for x in subj_ids}\n",
    "\n",
    "for subj_id in subj_ids:\n",
    "    # Set paths\n",
    "    behav_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/behav/Day1'\n",
    "\n",
    "    # Find the timestamps of ONSET and OFFSET of all the sync signals in the photodiode \n",
    "    # moving average helps us detect the deflections \n",
    "    sig = np.squeeze(sync_utils.moving_average(photodiode_dict[subj_id][0]._data, n=11))\n",
    "    timestamp = np.squeeze(np.arange(len(sig))/mne_dict[subj_id][0].info['sfreq'])\n",
    "    # normalize\n",
    "    sig =  zscore(sig)\n",
    "    # look for z-scores above 1\n",
    "    trig_ix = np.where((sig[:-1]<=0)*(sig[1:]>0))[0] # rising edge of trigger\n",
    "    neural_ts = timestamp[trig_ix]\n",
    "    neural_ts = np.array(neural_ts)\n",
    "    print(f'There are {len(neural_ts)} neural syncs detected')\n",
    "    \n",
    "    # Get the .log file and/or .csv file, depending on how your task logs the behavioral data. Eventually this should be fairly standardized across tasks.\n",
    "\n",
    "    log_path = glob(f'{behav_path}/*.log')[0]\n",
    "    csv_path = glob(f'{behav_path}/*MB_MEM*.csv')[0]\n",
    "    \n",
    "    # Now get the relevant timestamps from behavioral logfiles. This will differ depending\n",
    "\n",
    "    MB1_ts = {'trial_start': [], \n",
    "    'deck_start': [], \n",
    "    'feedback_start': [],\n",
    "    'ITI_start': [],\n",
    "    'ITI_stop': []}\n",
    "\n",
    "    MEM2_ts = {'trial_start': [], \n",
    "    'face_start': [], \n",
    "    'slider_start': [],\n",
    "    'slider_stop': [],\n",
    "    'ITI_start': [],\n",
    "    'ITI_stop': []}\n",
    "\n",
    "    beh_ts = []\n",
    "\n",
    "    MB1_FLAG = True \n",
    "    MEM2_FLAG = False \n",
    "\n",
    "    with open(log_path, 'r') as fobj:\n",
    "        for ix, line in enumerate(fobj.readlines()):\n",
    "            line = line.replace('\\r', '')\n",
    "            tokens = line[:-1].split('\\t')\n",
    "\n",
    "            if tokens[1] == 'EXP ':\n",
    "                # Determine which task we are looking at \n",
    "                if tokens[2][0:3] == 'MB1':\n",
    "                    MB1_FLAG = True\n",
    "                    MEM2_FLAG = False \n",
    "                elif tokens[2][0:3] == 'MEM':\n",
    "                    MEM2_FLAG = True\n",
    "                    MB1_FLAG = False\n",
    "\n",
    "                # Grab photodiode timestamp\n",
    "                if tokens[2][0:4] =='sync':\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        beh_ts.append(float(tokens[0]))\n",
    "\n",
    "                # Get MB1 deck \n",
    "                if 'MB1_left_draw' in tokens[2]:\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MB1_ts['deck_start'].append(float(tokens[0]))\n",
    "\n",
    "                # Get MB1 feedback\n",
    "                if 'MB1_face' in tokens[2]:\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MB1_ts['feedback_start'].append(float(tokens[0]))\n",
    "\n",
    "                # Get MB1 ITI cross \n",
    "                if 'MB1_ITI_cross' in tokens[2]:\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MB1_ts['ITI_start'].append(float(tokens[0]))\n",
    "                    elif 'autoDraw = False' in tokens[2]:\n",
    "                        MB1_ts['ITI_stop'].append(float(tokens[0]))\n",
    "\n",
    "                if 'New trial (rep=0' in tokens[2]:\n",
    "                    if MB1_FLAG: \n",
    "                        # remember to discard the first one later - it's pre-session \n",
    "                        MB1_ts['trial_start'].append(float(tokens[0]))\n",
    "                    elif MEM2_FLAG:\n",
    "                        MEM2_ts['trial_start'].append(float(tokens[0]))\n",
    "\n",
    "                # Get MEM2 ITI\n",
    "                if 'MEM2_jitter' in tokens[2]:\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MEM2_ts['ITI_start'].append(float(tokens[0]))          \n",
    "                    elif 'autoDraw = False' in tokens[2]:\n",
    "                        MEM2_ts['ITI_stop'].append(float(tokens[0]))  \n",
    "\n",
    "                # Get MEM2 Face\n",
    "                if 'MEM2_images' in tokens[2]:\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MEM2_ts['face_start'].append(float(tokens[0]))     \n",
    "\n",
    "                # Get MEM2 slider start\n",
    "                if tokens[2][:16] == 'MEM2_conf_slider':\n",
    "                    if 'autoDraw = True' in tokens[2]:\n",
    "                        MEM2_ts['slider_start'].append(float(tokens[0]))    \n",
    "\n",
    "                 # Get MEM2 slider stop\n",
    "                if tokens[2][:16] == 'MEM2_conf_slider':\n",
    "                    if 'autoDraw = False' in tokens[2]:\n",
    "                        MEM2_ts['slider_stop'].append(float(tokens[0]))                                               \n",
    "\n",
    "    beh_ts = np.array(beh_ts)\n",
    "    print(f'There are {len(beh_ts)} behav syncs detected')\n",
    "\n",
    "    # Note: fixation crosses need fixing on stop time duplicates\n",
    "    MB1_ts['ITI_stop'] = np.unique(MB1_ts['ITI_stop']).tolist()\n",
    "    MEM2_ts['ITI_stop'] = np.unique(MEM2_ts['ITI_stop']).tolist()\n",
    "\n",
    "    # Get the choice times: \n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "    MB1_ts['choice'] = (csv_data['MB1_draw_key.started'].dropna() + csv_data['MB1_draw_key.rt'].dropna()).tolist()\n",
    "    MEM2_ts['choice'] = (csv_data['MEM2_recall_key.started'].dropna() + csv_data['MEM2_recall_key.rt'].dropna()).tolist()\n",
    "\n",
    "    # Do some corrections: \n",
    "    # Get rid of first trial start (pre-session)\n",
    "    MB1_ts['trial_start'].pop(0) \n",
    "    \n",
    "    subj_count = 0\n",
    "\n",
    "    # Load the database with image DPRIME information\n",
    "    database_file = f'{behav_path}/all_mem_data.xlsx' \n",
    "    all_mem_df = pd.read_excel(database_file, engine='openpyxl')\n",
    "    all_mem_df = all_mem_df[['img_path', 'DPRIME']]\n",
    "\n",
    "    # Turn into right format for modeling: \n",
    "    MB1_n = 60\n",
    "    MEM2_n = 120\n",
    "    li_mb1 = []\n",
    "    li_mem2 = [] \n",
    "\n",
    "    act_rew_rate = {}\n",
    "    act_rew_rate['pids'] = []\n",
    "\n",
    "    r1_chance=30\n",
    "\n",
    "    for elem in ['actions', 'rewards']:\n",
    "        act_rew_rate[elem] = np.zeros(MB1_n).astype(int) # len(task_files), \n",
    "\n",
    "    # Load the merged task data \n",
    "    csv_data['trials_2.thisN'] = csv_data['trials_2.thisRepN'].shift(-1)\n",
    "\n",
    "    ##### First, process the Bandit task: \n",
    "    mb_df = csv_data.dropna(subset=['trials_2.thisN'])\n",
    "    act_rew_rate['pids'].append(mb_df.participant.iloc[0])\n",
    "\n",
    "    # Change Gender so that female = 2\n",
    "    mb_df.Gender[mb_df.Gender==0] = 2\n",
    "\n",
    "    # add score, reward probability and expected value \n",
    "    mb_df['choice'] = mb_df.apply(lambda x: x['MB1_draw_key.keys'], axis=1)\n",
    "    # Make the trials 1-60 \n",
    "    mb_df['trials_dm'] = mb_df['trials_2.thisN'].shift(+1)\n",
    "    # mb_df.trials_dm.fillna(60, inplace=True)\n",
    "    # # get rid of the extra rows in the .csv that populate between trials \n",
    "    mb_df = mb_df.drop_duplicates(subset='trials_dm', keep='first')\n",
    "    mb_df['reward'] = mb_df.apply(lambda x: x['reward']/100, axis=1)\n",
    "    mb_df.reward[mb_df.reward==100] = 0\n",
    "    # 0 is male, 1 is female \n",
    "    mb_df['choice'] = mb_df['choice']-1\n",
    "    mb_df.rename(columns={'MB1_draw_key.rt':'draw_rt'}, inplace=True)\n",
    "    mb_df.dropna(subset=['img_path'], inplace=True)\n",
    "\n",
    "    ##### Fit RW model to DM data\n",
    "    mb_df['bic'] = np.nan\n",
    "    mb_df['alpha']  = np.nan\n",
    "    mb_df['beta']  = np.nan\n",
    "    mb_df['RPE']  = np.nan\n",
    "\n",
    "    # RW_model = RW() \n",
    "    sub_act_rew_rate = {}\n",
    "    sub_act_rew_rate['pids'] = np.array([subj_count])  \n",
    "\n",
    "    for elem in ['actions', 'rewards']:\n",
    "        sub_act_rew_rate[elem] = np.zeros([1, MB1_n]).astype(int)\n",
    "    c = mb_df.choice.dropna().values.astype(int)\n",
    "    r = mb_df.reward.dropna().values\n",
    "    sub_act_rew_rate['actions'][0, :] = c\n",
    "    sub_act_rew_rate['rewards'][0, :] = r\n",
    "\n",
    "    # Save dict for modeling decision-making performance: \n",
    "    c = mb_df.choice.dropna().values.astype(int)\n",
    "    r = mb_df.reward.dropna().values\n",
    "    act_rew_rate['actions'] = c # [subj_count, :]\n",
    "    act_rew_rate['rewards']= r # [subj_count, :]\n",
    "\n",
    "    ##### Second, process the MEM2 data: \n",
    "    rm_df = csv_data.dropna(subset=['MEM2_trials.thisN'])\n",
    "\n",
    "    # add coding of memory choice: \n",
    "    rm_df['hit'] = 0\n",
    "    rm_df['miss'] = 0\n",
    "    rm_df['corr_reject'] = 0\n",
    "    rm_df['false_alarm'] = 0\n",
    "\n",
    "    # add score, reward probability and expected value \n",
    "    rm_df.rename(columns={'MEM2_recall_key.keys': 'response',\n",
    "    'MEM2_conf_slider.response': 'confidence'\n",
    "     }, inplace=True) \n",
    "\n",
    "    rm_df['trials_mem'] = rm_df['MEM2_trials.thisN'].shift(-1)\n",
    "    rm_df.trials_mem.fillna(120, inplace=True)\n",
    "\n",
    "    # Change Gender so that female = 2\n",
    "    rm_df.Gender[rm_df.Gender==0] = 2\n",
    "\n",
    "    rm_df = rm_df.merge(mb_df, on='img_path', how='left', indicator=True)\n",
    "    # Clean up the merge\n",
    "    rm_df.drop(columns=['participant_y'], inplace=True)\n",
    "    rm_df.rename(columns={'participant_x': 'participant'}, inplace=True)\n",
    "\n",
    "    hit_bool = (rm_df._merge=='both') & (rm_df.response==2)\n",
    "    hits = hit_bool.sum()\n",
    "    # NEW = 1, which is false in this case \n",
    "    miss_bool = (rm_df._merge=='both') & (rm_df.response==1)\n",
    "    misses = miss_bool.sum()\n",
    "\n",
    "    # or just the \"left\" df ('new')\n",
    "    false_alarm_bool = (rm_df._merge=='left_only') & (rm_df.response==2)\n",
    "    false_alarms = false_alarm_bool.sum()\n",
    "    corr_reject_bool = (rm_df._merge=='left_only') & (rm_df.response==1)\n",
    "    correct_rejections = corr_reject_bool.sum()\n",
    "\n",
    "    # categorize image by memory choice\n",
    "    rm_df.hit[hit_bool] = 1\n",
    "    rm_df.miss[miss_bool] = 1\n",
    "    rm_df.false_alarm[false_alarm_bool] = 1\n",
    "    rm_df.corr_reject[corr_reject_bool] = 1\n",
    "\n",
    "    # compute dprime for the subject\n",
    "    hm = hits+misses\n",
    "    fc = false_alarms+correct_rejections\n",
    "\n",
    "    hrs = hits/hm\n",
    "    fars = false_alarms/fc\n",
    "\n",
    "    # Adjust extreme hit-rates or false-alarms\n",
    "    if hrs == 0: \n",
    "        hrs = 0.5/hm\n",
    "    elif hrs ==1: \n",
    "        hrs = (hm-0.5)/hm\n",
    "    if fars == 0: \n",
    "        fars = 0.5/fc\n",
    "    elif fars ==1: \n",
    "        fars = (fc-0.5)/fc\n",
    "\n",
    "    dp = dprime(hrs, fars, hm, fc)\n",
    "\n",
    "    # Add in subject-level memory characteristics (\"rates\")\n",
    "    rm_df['hit_rate'] = hrs\n",
    "    rm_df['false_alarm_rate'] = fars\n",
    "    rm_df['subj_dprime'] = np.nan\n",
    "    if dp != float(\"-inf\"):\n",
    "        rm_df['subj_dprime'] = dp    \n",
    "\n",
    "    # Merge in the image DPRIME \n",
    "    rm_df['DPRIME'] = rm_df.merge(all_mem_df, on='img_path', how='right')['DPRIME']\n",
    "    mb_df['DPRIME'] = mb_df.merge(all_mem_df, on='img_path', how='right')['DPRIME']\n",
    "    mb_df.rename(columns={'DPRIME': 'image_dprime'}, inplace=True) \n",
    "\n",
    "    rm_df.rename(columns={'DPRIME': 'image_dprime',\n",
    "    'Gender_x': 'image_gender',\n",
    "    'MEM2_recall_key.rt_x': 'recall_rt',\n",
    "    'MEM2_conf_slider.rt_x': 'slider_rt'\n",
    "    }, inplace=True) \n",
    "\n",
    "\n",
    "    # dm_df = pd.concat(li_mb1, axis=0, ignore_index=True)\n",
    "    mb_df['male'] = 0\n",
    "    mb_df['female'] = 0\n",
    "    mb_df.male = mb_df.apply(lambda x: 1 if x.choice==0 else 0, axis=1)\n",
    "    mb_df.female = mb_df.apply(lambda x: 1 if x.choice==1 else 0, axis=1)\n",
    "\n",
    "    rm_df['phit'] = rm_df.response - 1\n",
    "\n",
    "    col_mask = ((rm_df.columns.str.startswith('MEM')) | (rm_df.columns.str.startswith('MB')) | (rm_df.columns.str.startswith('trials.')) | (rm_df.columns.str.startswith('trials_2')) | (rm_df.columns.str.endswith('_y')))\n",
    "    rm_df = rm_df.loc[:,~col_mask]\n",
    "\n",
    "    # Do regression to find neural timestamps for each event type\n",
    "    if len(beh_ts)!=len(neural_ts):\n",
    "        good_beh_ms, neural_offset = sync_utils.pulsealign(beh_ts, neural_ts, window=50, thresh=0.95)\n",
    "        slope, offset, rval = sync_utils.sync_matched_pulses(good_beh_ms, neural_offset)\n",
    "    else:\n",
    "        slope, offset, rval = sync_utils.sync_matched_pulses(beh_ts, neural_ts)\n",
    "\n",
    "    if rval < 0.99:\n",
    "        print('sync failed')\n",
    "    else: \n",
    "        print('sync succeeded')\n",
    "        \n",
    "    slopes[subj_id].append(slope)\n",
    "    offsets[subj_id].append(offset)\n",
    "    bandit_evs[subj_id].append(MB1_ts)\n",
    "    memory_evs[subj_id].append(MEM2_ts)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make epochs and remove IEDs. Currently just doing this for one example period - when subjects receive feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /sc/arion/work/qasims01/MemoryBanditData/EMU/Subjects/MS007/wm_ref_ieeg.fif...\n",
      "    Range : 0 ... 1867007 =      0.000 ...  1823.249 secs\n",
      "Ready.\n",
      "Reading 0 ... 1867007  =      0.000 ...  1823.249 secs...\n",
      "Used Annotations descriptions: ['feedback_start']\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 4609 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/lfp_preprocess_utils.py:801: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  elec_df['label'] = mne_data_reref.ch_names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Setting up band-pass filter from 25 - 80 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 25.00\n",
      "- Lower transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 21.88 Hz)\n",
      "- Upper passband edge: 80.00 Hz\n",
      "- Upper transition bandwidth: 20.00 Hz (-6 dB cutoff frequency: 90.00 Hz)\n",
      "- Filter length: 271 samples (0.529 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 5733 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 6840 out of 6840 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 114 out of 114 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 114 columns\n",
      "Overwriting existing file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123865/2170666864.py:31: RuntimeWarning: This filename (/sc/arion/work/qasims01/MemoryBanditData/EMU/Subjects/MS007/epoch_feedback_start.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs_all_evs[event].save(f'{save_path}/epoch_{event}.fif', overwrite=True)\n"
     ]
    }
   ],
   "source": [
    "# set some windows of interest \n",
    "\n",
    "buf = 1.0 # this is the buffer before and after that we use to limit edge effects for TFRs\n",
    "\n",
    "IED_args = {'peak_thresh':4,\n",
    "           'closeness_thresh':0.25, \n",
    "           'width_thresh':0.2}\n",
    "\n",
    "evs = ['feedback_start']\n",
    "\n",
    "# add behavioral times of interest \n",
    "for subj_id in subj_ids:\n",
    "    # Set paths\n",
    "    load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "    save_path = f'{base_dir}/work/qasims01/MemoryBanditData/EMU/Subjects/{subj_id}'\n",
    "\n",
    "    epochs_all_evs = {f'{x}': np.nan for x in evs}\n",
    "    for event in evs:\n",
    "        pre = 1\n",
    "        post = 1.5\n",
    "        fixed_baseline = (-1.0, 0)\n",
    "\n",
    "        epochs = lfp_preprocess_utils.make_epochs(load_path=f'{save_path}/wm_ref_ieeg.fif',\n",
    "                                                  elec_data=elec_dict[subj_id][0], \n",
    "                                                  slope=slopes[subj_id][0], offset=offsets[subj_id][0], \n",
    "                                                  behav_name = event, behav_times=bandit_evs[subj_id][0][event], \n",
    "                                                  baseline_times=None, baseline_dur=None, fixed_baseline=fixed_baseline,\n",
    "                                                  buf_s=buf, pre_s=-pre, post_s=post, downsamp_factor=2, IED_args=IED_args)\n",
    "\n",
    "        epochs_all_evs[event] = epochs\n",
    "        epochs_all_evs[event].save(f'{save_path}/epoch_{event}.fif', overwrite=True)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive analyses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are mostly going to make a lot of spectral plots and things to look over manually as a function of event timing, without getting into the more advanced GLM stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /sc/arion/work/qasims01/MemoryBanditData/EMU/Subjects/MS007/epoch_feedback_start.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -2000.00 ...    2498.05 ms\n",
      "        0 CTF compensation matrices available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_123865/4117495012.py:8: RuntimeWarning: This filename (/sc/arion/work/qasims01/MemoryBanditData/EMU/Subjects/MS007/epoch_feedback_start.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(f'{load_path}', preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 114 columns\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Replacing existing metadata with 117 columns\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "evs = ['feedback_start']\n",
    "epochs_all_subjs_all_evs = {f'{a}': {f'{b}': np.nan for b in evs} for a in subj_ids}\n",
    "\n",
    "for subj_id in subj_ids:\n",
    "    for event in evs:\n",
    "        load_path = f'{base_dir}/work/qasims01/MemoryBanditData/EMU/Subjects/{subj_id}/epoch_{event}.fif'\n",
    "        epochs = mne.read_epochs(f'{load_path}', preload=True)\n",
    "        epochs_all_subjs_all_evs[subj_id][event] = epochs\n",
    "    \n",
    "        # Let's add our behavioral events to the metadata. Substitute this with the particulars for your behavioral dataframe. \n",
    "\n",
    "        event_metadata = epochs_all_subjs_all_evs[subj_id][event].metadata.copy()\n",
    "\n",
    "        event_metadata['rt'] = mb_df['draw_rt'].tolist()\n",
    "        event_metadata['reward'] = mb_df['reward'].tolist()\n",
    "        event_metadata['dprime'] = mb_df['image_dprime'].tolist()\n",
    "\n",
    "        epochs_all_subjs_all_evs[subj_id][event].metadata = event_metadata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        \n",
       "        <td>feedback_start: 60</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>-2.000 – 2.498 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>-1.000 – 0.000 sec</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<EpochsFIF |  60 events (all good), -2 - 2.49805 sec, baseline -1 – 0 sec, ~120.4 MB, data loaded, with metadata,\n",
       " 'feedback_start': 60>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs_all_subjs_all_evs['MS007']['feedback_start']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and examine the epochs if you'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# fig = epochs_all_subjs_all_evs['MS007']['feedback_start'].plot(n_epochs=10, n_channels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need this following line to save the annotations to the epochs object \n",
    "# fig.fake_keypress('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power spectrum (FOOOF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to plot the group-level FOOOF analysis for two conditions in the task across all channels, and then also plot the individual power spectrums for each channel with model fit for each of the two conditions. Note the time specification to make sure I leave out the baseline period... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sc/arion/work/qasims01/test-env/envs/LFPAnalysis/lib/python3.10/site-packages/fooof/objs/group.py:378: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  out = np.array([np.insert(getattr(data, name), 3, index, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sc/arion/work/qasims01/test-env/envs/LFPAnalysis/lib/python3.10/site-packages/fooof/objs/group.py:378: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  out = np.array([np.insert(getattr(data, name), 3, index, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Set PSD parameters: \n",
    "fmax = 250\n",
    "tmin = 0\n",
    "tmax = 1.5\n",
    "\n",
    "# Define the frequency range to fit\n",
    "freq_range = [1, 30]\n",
    "\n",
    "# Define peak_width, peak_height, peak_threshold, max_n_peaks\n",
    "peak_width_limits = [1, 8]\n",
    "min_peak_height = 0.1\n",
    "peak_threshold = 1. \n",
    "max_n_peaks = 6\n",
    "\n",
    "# Use the metadata to parse the epochs before computing anything\n",
    "\n",
    "data_parsing = ['reward==1',\n",
    "                'reward==0']\n",
    "\n",
    "fooof_groups = {f'{x}': np.nan for x in data_parsing}\n",
    "\n",
    "for subj_id in subj_ids:\n",
    "    for event in evs:\n",
    "        \n",
    "        epochs = epochs_all_subjs_all_evs[subj_id][event]\n",
    "        \n",
    "        for parsing in data_parsing: \n",
    "            \n",
    "            file_path = f'{base_dir}/work/qasims01/MemoryBanditData/EMU/Subjects/{subj_id}/scratch/FOOOF/{event}'\n",
    "            # If the path doesn't exist, make it:\n",
    "            if not os.path.exists(file_path): \n",
    "                os.makedirs(file_path)\n",
    "                \n",
    "            file_name = f'group_{parsing}'\n",
    "            \n",
    "            epo_spectrum = epochs[parsing].compute_psd(method='multitaper',\n",
    "                                                                fmax=fmax,\n",
    "                                                                tmin=tmin,\n",
    "                                                                tmax=tmax)\n",
    "            psds, freqs = epo_spectrum.get_data(return_freqs=True)\n",
    "\n",
    "            # average across epochs\n",
    "            psd_trial_avg = np.average(psds, axis=0)  # (3, 513)\n",
    "\n",
    "\n",
    "            # Initialize a FOOOFGroup object, with desired settings\n",
    "            fg = FOOOFGroup(peak_width_limits=peak_width_limits, \n",
    "                            min_peak_height=min_peak_height,\n",
    "                            peak_threshold=peak_threshold, \n",
    "                            max_n_peaks=max_n_peaks, \n",
    "                            verbose=False)\n",
    "\n",
    "            fg.fit(freqs, psd_trial_avg, freq_range)\n",
    "\n",
    "            fooof_groups[parsing] = fg\n",
    "\n",
    "            # Check the overall results of the group fits\n",
    "            fg.save_report(file_name=file_name,\n",
    "                          file_path=file_path)\n",
    "\n",
    "            # Check the overall results for each channel: \n",
    "            for chan in range(psd_trial_avg.shape[0]): \n",
    "                \n",
    "                file_name = f'{epo_spectrum.ch_names[chan]}_{parsing}'\n",
    "                \n",
    "                fm = fg.get_fooof(ind=0, regenerate=True)\n",
    "                fm.save_report(file_name=file_name,\n",
    "                      file_path=file_path)\n",
    "                \n",
    "        # Save out the FOOOF objects\n",
    "        joblib.dump(fooof_groups, f'{file_path}/FOOOF_objects.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For guidance on picking the parameters used to fit: https://fooof-tools.github.io/fooof/auto_tutorials/plot_07-TroubleShooting.html#sphx-glr-auto-tutorials-plot-07-troubleshooting-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet TFRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will plot some basic TFRs of our behaviorally-locked analysis using a wavelet transform. Let's set some parameters first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the metadata to assign conditions to parse your epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power parameters \n",
    "freqs = np.logspace(*np.log10([4, 128]), num=20)\n",
    "n_cycles = 4  \n",
    "sr = epochs_all_subjs_all_evs[subj_id][event].info['sfreq']\n",
    "buf = 1.0\n",
    "buf_ix = int(buf*sr)\n",
    "      \n",
    "data_parsing = ['reward==1',\n",
    "                'reward==0']\n",
    "\n",
    "for subj_id in subj_ids:\n",
    "    for event in evs:\n",
    "                \n",
    "        diff_pow_epochs = {f'{x}':np.nan for x in data_parsing}\n",
    "\n",
    "        epochs = epochs_all_subjs_all_evs[subj_id][event]\n",
    "        \n",
    "        good_chans = [x for x in epochs.ch_names if x not in epochs.info['bads']]\n",
    "        picks = [x for x in good_chans]\n",
    "        \n",
    "        \n",
    "        # baseline_pow = mne.time_frequency.tfr_morlet(baseline_epochs, picks=picks,\n",
    "        #                                      freqs=freqs, n_cycles=n_cycles, use_fft=True,\n",
    "        #                 return_itc=False, n_jobs=-1, average=False)\n",
    "\n",
    "        # Compute power without averaging over events\n",
    "        \n",
    "        for parsing in data_parsing: \n",
    "            data_struct = np.ones([epochs[parsing]._data.shape[0], \n",
    "                                   epochs[parsing]._data.shape[1], len(freqs), \n",
    "                                   epochs[parsing]._data.shape[-1]])\n",
    "            \n",
    "            for ch_ix in np.arange(epochs._data.shape[1]): \n",
    "                ch_data = epochs[parsing]._data[:, ch_ix:ch_ix+1, :]\n",
    "                bad_epochs  = np.where(epochs.metadata.query(parsing)[epochs.ch_names[ch_ix]].notnull())[0]\n",
    "        #         print(len(bad_epochs))\n",
    "                good_epochs = np.delete(np.arange(ch_data.shape[0]), bad_epochs)\n",
    "                ch_data = np.delete(ch_data, bad_epochs, axis=0)\n",
    "                ch_pow = mne.time_frequency.tfr_array_morlet(ch_data, sfreq=epochs.info['sfreq'], \n",
    "                                                    freqs=freqs, n_cycles=n_cycles, zero_mean=True, \n",
    "                                                    use_fft=True, output='power', \n",
    "                                                    n_jobs=1)\n",
    "\n",
    "                data_struct[good_epochs, ch_ix, :, :] = ch_pow[:, 0, :, :]\n",
    "            temp_pow = mne.time_frequency.EpochsTFR(epochs[parsing].info, data_struct, \n",
    "                                                    epochs.times, freqs)\n",
    "\n",
    "        #     temp_pow = mne.time_frequency.tfr_morlet(epochs[parsing], picks=picks,\n",
    "        #                                              freqs=freqs, n_cycles=n_cycles, use_fft=True,\n",
    "        #                         return_itc=False, n_jobs=-1, average=False)\n",
    "            # Baseline correct the TFRs\n",
    "            temp_pow = temp_pow.apply_baseline(baseline=(-feedback_pre, 0), mode='zscore')\n",
    "        #     baseline_pow.crop(tmin=-baseline_pre, tmax=baseline_post)\n",
    "        #     temp_pow.data = lfp_preprocess_utils.zscore_TFR_across_trials(temp_pow.data, baseline_pow.data)\n",
    "            temp_pow.crop(tmin=-feedback_pre, tmax=feedback_post)\n",
    "            diff_pow_epochs[parsing] = temp_pow\n",
    "            \n",
    "            \n",
    "            # Make some plots \n",
    "            # TODO\n",
    "            \n",
    "            # Save out the data \n",
    "            # TODO \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analyses of TFRs:\n",
    "\n",
    "At this stage, you should **heavily** brainstorm the statistics you want to do, before you start writing any code. What is the goal of your analysis? What would actually allow you to show what you want to show?\n",
    "\n",
    "As an example, let's say I want to compare the reward vs. no-reward conditions for every channel, and identify the timepoints and frequencies that exhibit significant differences between conditions. To do so, I would utilize a non-parametric cluster-permutation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in diff_pow_epochs[parsing].ch_names[0:1]: \n",
    "    region_label = elec_df[elec_df.label==label].YBA_1.values[0]\n",
    "    if region_label == 'Unknown':\n",
    "        region_label = elec_df[elec_df.label==label]['Manual Examination'].values[0]\n",
    "\n",
    "    plot_ix = diff_pow_epochs[parsing].ch_names.index(label)\n",
    "    \n",
    "    rdata = diff_pow_epochs['reward==1'].data[:, plot_ix, :, :]\n",
    "    nrdata = diff_pow_epochs['reward==0'].data[:, plot_ix, :, :]\n",
    "\n",
    "for label in diff_pow_epochs[parsing].ch_names: \n",
    "    region_label = elec_df[elec_df.label==label].YBA_1.values[0]\n",
    "    if region_label == 'Unknown':\n",
    "        region_label = elec_df[elec_df.label==label]['Manual Examination'].values[0]\n",
    "\n",
    "    plot_ix = diff_pow_epochs[parsing].ch_names.index(label)\n",
    "    \n",
    "    rdata = diff_pow_epochs['reward==1'].data[:, plot_ix, :, :]\n",
    "    rdata = rdata[np.unique(np.where(~np.isnan(rdata))[0]), :, :]\n",
    "    nrdata = diff_pow_epochs['reward==0'].data[:, plot_ix, :, :]\n",
    "    nrdata = nrdata[np.unique(np.where(~np.isnan(nrdata))[0]), :, :]\n",
    "        \n",
    "    X = [rdata, \n",
    "         nrdata]\n",
    "    \n",
    "    F_obs, clusters, cluster_p_values, H0 = \\\n",
    "    mne.stats.permutation_cluster_test(X, n_permutations=1000, out_type='mask', \n",
    "                                       verbose=True)\n",
    "    \n",
    "    if any(cluster_p_values<=0.05):\n",
    "        print(region_label)\n",
    "        # Create new stats image with only significant clusters\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "        times = diff_pow_epochs['reward==0'].times\n",
    "\n",
    "        evoked_power_1 = np.nanmean(X[0], axis=0)\n",
    "        evoked_power_2 = np.nanmean(X[1], axis=0)\n",
    "        evoked_power_contrast = evoked_power_1 - evoked_power_2\n",
    "        signs = np.sign(evoked_power_contrast)\n",
    "\n",
    "        F_obs_plot = np.nan * np.ones_like(F_obs)\n",
    "        for c, p_val in zip(clusters, cluster_p_values):\n",
    "            if p_val <= 0.05:\n",
    "                F_obs_plot[c] = F_obs[c] * signs[c]\n",
    "\n",
    "        ax.imshow(F_obs,\n",
    "                  extent=[times[0], times[-1], freqs[0], freqs[-1]],\n",
    "                  aspect='auto', origin='lower', cmap='gray')\n",
    "        max_F = np.nanmax(abs(F_obs_plot))\n",
    "        ax.imshow(F_obs_plot,\n",
    "                  extent=[times[0], times[-1], freqs[0], freqs[-1]],\n",
    "                  aspect='auto', origin='lower', cmap='RdBu_r',\n",
    "                  vmin=-max_F, vmax=max_F)\n",
    "\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Frequency (Hz)')\n",
    "        # # ax.set_title(f'Induced power ({ch_name})')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp_analysis",
   "language": "python",
   "name": "lfpanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
